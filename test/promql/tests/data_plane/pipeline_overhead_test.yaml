evaluation_interval: 1m

rule_files:
  - prometheus.pipeline_alerts.yaml

tests:
  # ----- Pipeline Schedule Alerting Tests -----
  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      # Tekton is experiencing scheduling overhead == total execution, so it will be alerted
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="succeded",namespace="bar", source_cluster="cluster01"}'
        values: '2+2x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="succeded",namespace="bar", source_cluster="cluster01"}'
        values: '2+2x780'
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="failed",namespace="far", source_cluster="cluster01"}'
        values: '0.3+1x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="failed",namespace="far", source_cluster="cluster01"}'
        values: '1+1x780'

    alert_rule_test:
      - eval_time: 13h
        alertname: HighSchedulingOverhead
        exp_alerts:
          - exp_labels:
              severity: critical
              slo: "true"
              source_cluster: cluster01
            exp_annotations:
              summary: >-
                Tekton has a scheduling overhead vs. PipelineRun durations of <95%
              description: >-
                Tekton controller on cluster cluster01 factoring out the time needed to receive PipelineRun creation
                events from the overall PipelineRun execution time is at 0% instead of 95% or greater.
              alert_team_handle: <!subteam^S04PYECHCCU>
              team: pipelines
              runbook_url: https://gitlab.cee.redhat.com/konflux/docs/sop/-/blob/main/pipeline-service/slos/scheduling-overhead.md

  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      # Tekton overhead ratio is above 95%, should not alert
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="succeded",namespace="boo", source_cluster="cluster01"}'
        values: '0.5x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="succeded",namespace="boo", source_cluster="cluster01"}'
        values: '1x780'
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="succeded",namespace="far", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="succeded",namespace="far", source_cluster="cluster01"}'
        values: '600+600x780'
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="failed",namespace="far", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="failed",namespace="far", source_cluster="cluster01"}'
        values: '10000+10000x780'

    alert_rule_test:
      - eval_time: 13h
        alertname: HighSchedulingOverhead
        exp_alerts:
          - exp_labels:
              severity: critical
              slo: "true"
              source_cluster: cluster01
            exp_annotations:
              summary: >-
                Tekton has a scheduling overhead vs. PipelineRun durations of <95%
              description: >-
                Tekton controller on cluster cluster01 factoring out the time needed to receive PipelineRun creation
                events from the overall PipelineRun execution time is at 50% instead of 95% or greater.
              alert_team_handle: <!subteam^S04PYECHCCU>
              team: pipelines
              runbook_url: https://gitlab.cee.redhat.com/konflux/docs/sop/-/blob/main/pipeline-service/slos/scheduling-overhead.md

  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      # Tekton overhead ratio is above 95%, should not alert
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="succeded",namespace="boo", source_cluster="cluster01"}'
        values: '0.03x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="succeded",namespace="boo", source_cluster="cluster01"}'
        values: '1x780'
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="succeded",namespace="far", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="succeded",namespace="far", source_cluster="cluster01"}'
        values: '10000+10000x780'
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="failed",namespace="foo", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="failed",namespace="foo", source_cluster="cluster01"}'
        values: '1+1x780'

    alert_rule_test:
      - eval_time: 13h
        alertname: HighSchedulingOverhead

  # ----- Pipeline Execution Alerting Tests -----
  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      # Tekton is experiencing execution overhead == total execution, so it will be alerted
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="succeded",namespace="bar", source_cluster="cluster01"}'
        values: '2+2x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="succeded",namespace="bar", source_cluster="cluster01"}'
        values: '2+2x780'
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="failed",namespace="far", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="failed",namespace="far", source_cluster="cluster01"}'
        values: '10000+10000x780'

    alert_rule_test:
      - eval_time: 13h
        alertname: HighExecutionOverhead
        exp_alerts:
          - exp_labels:
              severity: critical
              slo: "true"
              source_cluster: cluster01
            exp_annotations:
              summary: >-
                Tekton has a execution overhead vs. PipelineRun durations of <95%
              description: >-
                Tekton controller on cluster cluster01 factoring out the time needed to create
                underlying TaskRuns from the overall PipelineRun execution time is at 0% instead of 95% or greater.
              alert_team_handle: <!subteam^S04PYECHCCU>
              team: pipelines
              runbook_url: https://gitlab.cee.redhat.com/konflux/docs/sop/-/blob/main/pipeline-service/slos/execution-overhead.md

  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      # Tekton execution overhead is better than 95%, should not alert
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '0.05263x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '1x780'
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '19+19x780'
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="succeded",namespace="far", source_cluster="cluster01"}'
        values: '2+2x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="succeded",namespace="far", source_cluster="cluster01"}'
        values: '38+38x780'
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="failed",namespace="foo", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="failed",namespace="foo", source_cluster="cluster01"}'
        values: '10000+10000x780'

    alert_rule_test:
      - eval_time: 13h
        alertname: HighExecutionOverhead
        exp_alerts:
          - exp_labels:
              severity: critical
              slo: "true"
              source_cluster: cluster01
            exp_annotations:
              summary: >-
                Tekton has a execution overhead vs. PipelineRun durations of <95%
              description: >-
                Tekton controller on cluster cluster01 factoring out the time needed to create
                underlying TaskRuns from the overall PipelineRun execution time is at 94.74% instead of 95% or greater.
              alert_team_handle: <!subteam^S04PYECHCCU>
              team: pipelines
              runbook_url: https://gitlab.cee.redhat.com/konflux/docs/sop/-/blob/main/pipeline-service/slos/execution-overhead.md

  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      # Tekton execution overhead is better than 95%, should not alert
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '0.03x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '1x780'
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '10000+10000x780'
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="succeded",namespace="bar", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="succeded",namespace="bar", source_cluster="cluster01"}'
        values: '10000+10000x780'
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="failed",namespace="bar", source_cluster="cluster01"}'
        values: '2+2x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="failed",namespace="bar", source_cluster="cluster01"}'
        values: '2+2x780'

    alert_rule_test:
      - eval_time: 13h
        alertname: HighExecutionOverhead

  # ----- Chains Controller Performance / High Overhead ----
  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      - series: 'watcher_client_latency_bucket{app="tekton-chains-controller", le="1", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'watcher_client_latency_bucket{app="tekton-chains-controller", le="10", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'watcher_client_latency_bucket{app="tekton-chains-controller", le="+Inf", source_cluster="cluster01"}'
        values: '900+900x780'
      - series: 'watcher_client_latency_bucket{app="tekton-pipelines-controller", le="1", source_cluster="cluster01"}'
        values: '1x780'
      - series: 'watcher_client_latency_bucket{app="tekton-pipelines-controller", le="+Inf", source_cluster="cluster01"}'
        values: '1x780'

    alert_rule_test:
      - eval_time: 85m
        alertname: ChainsControllerPerformance
        exp_alerts:
          - exp_labels:
              severity: critical
              slo: "true"
              source_cluster: cluster01
            exp_annotations:
              summary: >-
                Tekton chains controller performance appears degraded with uncommonly high client latency.
              description: >-
                Tekton chains controller on cluster cluster01 performance appears degraded with client latency 33.33%.
              alert_team_handle: <!subteam^S04PYECHCCU>
              team: pipelines
              runbook_url: https://gitlab.cee.redhat.com/konflux/docs/sop/-/blob/main/pipeline-service/slos/chains-performance.md

  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      - series: 'watcher_client_latency_bucket{app="tekton-chains-controller", le="1", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'watcher_client_latency_bucket{app="tekton-chains-controller", le="+Inf", source_cluster="cluster01"}'
        values: '400+400x780'

    alert_rule_test:
      - eval_time: 85m
        alertname: ChainsControllerPerformance
