evaluation_interval: 1m

rule_files:
  - prometheus.pipeline_alerts.yaml

tests:
  # ----- Results Too Many Errors ----
  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="OK", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="Unavailable", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="Internal", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="NotFound", source_cluster="cluster01"}'
        values: '1+1x780'

    alert_rule_test:
      - eval_time: 85m
        alertname: ResultsSuccessRateTooManyErrors
        exp_alerts:
          - exp_labels:
              severity: critical
              slo: "true"
              source_cluster: cluster01
            exp_annotations:
              summary: >-
                Tekton results is experiencing too many API errors.
              description: >-
                Tekton results on cluster cluster01 only has an API success rate of 25%.
              alert_team_handle: <!subteam^S04PYECHCCU>
              team: pipelines
              runbook_url: https://gitlab.cee.redhat.com/konflux/docs/sop/-/blob/main/pipeline-service/sre/tekton-result-high-errors.md
  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="OK", source_cluster="cluster01"}'
        values: '2+2x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="Unavailable", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="Internal", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="NotFound", source_cluster="cluster01"}'
        values: '0x780'

    alert_rule_test:
      - eval_time: 85m
        alertname: ResultsSuccessRateTooManyErrors
        exp_alerts:
          - exp_labels:
              severity: critical
              slo: "true"
              source_cluster: cluster01
            exp_annotations:
              summary: >-
                Tekton results is experiencing too many API errors.
              description: >-
                Tekton results on cluster cluster01 only has an API success rate of 50%.
              alert_team_handle: <!subteam^S04PYECHCCU>
              team: pipelines
              runbook_url: https://gitlab.cee.redhat.com/konflux/docs/sop/-/blob/main/pipeline-service/sre/tekton-result-high-errors.md

  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="OK", source_cluster="cluster01"}'
        values: '0x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="Unavailable", source_cluster="cluster01"}'
        values: '0x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="Internal", source_cluster="cluster01"}'
        values: '0x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="NotFound", source_cluster="cluster01"}'
        values: '0x780'

    alert_rule_test:
      - eval_time: 85m
        alertname: ResultsSuccessRateTooManyErrors

  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="OK", source_cluster="cluster01"}'
        values: '100+100x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="Unavailable", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="Internal", source_cluster="cluster01"}'
        values: '0x780'
      - series: 'grpc_server_handled_total{job="tekton-results-api", grpc_service="tekton.results.v1alpha2.CreateResult", grpc_code="NotFound", source_cluster="cluster01"}'
        values: '0x780'

    alert_rule_test:
      - eval_time: 85m
        alertname: ResultsSuccessRateTooManyErrors

  # ----- PAC Controller Performance / High Overhead ----
  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      - series: 'pac_watcher_client_latency_bucket{le="1", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pac_watcher_client_latency_bucket{le="10", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pac_watcher_client_latency_bucket{le="+Inf", source_cluster="cluster01"}'
        values: '900+900x780'
      - series: 'pac_watcher_client_latency_bucket{le="1", source_cluster="cluster02"}'
        values: '1x780'
      - series: 'pac_watcher_client_latency_bucket{le="+Inf", source_cluster="cluster02"}'
        values: '1x780'

  # ----- Chains Controller Overhead Test----
  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      # tekton chains controller is experiencing delays processing pipelineruns in tenant-1 started and should alert.  Also make sure the lack
      # of issues in another app does not affect the value of the query
      - series: 'watcher_workqueue_depth{container="tekton-chains-controller", app="tekton-chains-controller", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'watcher_workqueue_depth{container="tekton-pipelines-controller", app="tekton-pipelines-controller", source_cluster="cluster01"}'
        values: '0x780'

    alert_rule_test:
      - eval_time: 75m
        alertname: ChainsControllerOverhead
        exp_alerts:
          - exp_labels:
              severity: critical
              slo: "true"
              source_cluster: cluster01
            exp_annotations:
              summary: >-
                Tekton chains controller is experiencing high overhead in processing pipelineruns.
              description: >-
                Tekton chains controller on cluster cluster01 has recorded 600 pipelinerun events. This suggests potential performance degradation and a risk of an eventual deadlock condition.
              alert_team_handle: <!subteam^S04PYECHCCU>
              team: pipelines
              runbook_url: https://gitlab.cee.redhat.com/konflux/docs/sop/-/blob/main/pipeline-service/sre/tekton-pipeline-related-deadlocks.md

  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      # tekton chains controller is experiencing some delays processing pipelineruns but not fast enough so it should not alert
      - series: 'watcher_workqueue_depth{container="tekton-chains-controller", app="tekton-chains-controller", source_cluster="cluster01"}'
        values: '1x780'

    alert_rule_test:
      - eval_time: 10m
        alertname: ChainsControllerOverhead