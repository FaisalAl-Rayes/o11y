apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: rhtap-performance-alerting
  labels:
    tenant: rhtap
spec:
  groups:
    - name: performance_alerts
      interval: 1m
      rules:
        # ETCD alerts
        - alert: EtcdFsyncLatency
          expr: |
            avg_over_time(histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[2m]))[10m:]) > 0.01
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: >-
              ETCD slow file system synchronization.
            description: >-
              10 minutes avg. 99th etcd fsync latency on {{$labels.pod}} higher than 10ms.

        - alert: EtcdCommitLatency
          expr: |
            avg_over_time(histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[2m]))[10m:]) > 0.03
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: >-
              ETCD slow writes observed.
            description: >-
              10 minutes avg. 99th etcd commit latency on {{$labels.pod}} higher than 30ms.

        - alert: EtcdProposalFailures
          expr: |
            rate(etcd_server_proposals_failed_total[1h]) > 0.05
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: >-
              ETCD raft proposal failures.
            description: >-
              Etcd high number of failed proposals on pod {{ $labels.pod }}

        - alert: EtcdSlowNetworkRTT
          expr: |
            histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[5m])) > 0.1
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: >-
              High RTT latency on ETCD cluster member requests.
            description: >-
              99th etcd RTT latency rate on {{$labels.pod}} higher than 0.1

        - alert: EtcdSlowGRPC
          expr: |
            histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job=~".*etcd.*", grpc_method!="Defragment", grpc_type="unary"}[15m])) without(grpc_type)) > 0.15
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: >-
              ETCD slow requests.
            description: >-
              Slow GRPC request processing by ETCD server {{ $labels.pod }}.

        # KubeAPI Alerts
        - alert: KubeAPIHighLatency
          expr: |
            avg_over_time(histogram_quantile(0.99, sum(irate(apiserver_request_duration_seconds_bucket{apiserver="kube-apiserver", verb=~"LIST|GET", subresource!~"log|exec|portforward|attach|proxy", scope="cluster"}[5m])) by (le, resource, verb, scope))[5m:]) > 30
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: >-
              High Latency on Kube API Server Request.
            description: >-
              5 minutes avg. 99th read-only API call latency for {{$labels.verb}}/{{$labels.resource}} in scope {{$labels.scope}} higher than 30 seconds. {{$value}}s

        # Node based Alerts
        - alert: NodeHighCPU
          expr: |
            (100 * avg(1 - rate(node_cpu_seconds_total{mode="idle"}[5m])) by (instance)) > 95
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: >-
              Node High CPU Usage.
            description: >-
              CPU Usage is {{$value}}% on node {{ $labels.instance }}

        - alert: NodeHighMemory
          expr: |
            100*((node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes)/node_memory_MemTotal_bytes) > 90
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: >-
              Node High Memory Usage.
            description: >-
              Memory Usage is {{$value}}% on node {{ $labels.instance }}
